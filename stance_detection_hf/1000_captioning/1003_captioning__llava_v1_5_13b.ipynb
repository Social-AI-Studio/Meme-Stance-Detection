{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw_8I5QeMmqK"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBeYLjotxgQ"
      },
      "outputs": [],
      "source": [
        "# Pre-requisites. via\n",
        "# https://github.com/haotian-liu/LLaVA\n",
        "!git clone https://github.com/haotian-liu/LLaVA.git\n",
        "%cd LLaVA\n",
        "!conda create -n llava python=3.10 -y\n",
        "!conda activate llava\n",
        "!pip install --upgrade pip\n",
        "!pip install fastapi kaleido python-multipart typing-extensions==4.5.0 uvicorn\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRBlYYjoZHTZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# Import libraries. via\n",
        "# https://medium.com/supportvectors/comparing-image-to-text-models-blip-blip2-and-llava-ebd66d2eb6d8\n",
        "import torch\n",
        "from llava.serve.cli import load_image\n",
        "from llava.constants import IMAGE_TOKEN_INDEX\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0m8GEpXOtW9"
      },
      "source": [
        "# Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxkroyd1OuY0"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model. via\n",
        "# https://github.com/haotian-liu/LLaVA\n",
        "# https://medium.com/supportvectors/comparing-image-to-text-models-blip-blip2-and-llava-ebd66d2eb6d8\n",
        "model_path = 'liuhaotian/llava-v1.5-13b'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=model_path,\n",
        "    model_base=None,\n",
        "    model_name=get_model_name_from_path(model_path),\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzouBmV4M5iL"
      },
      "source": [
        "# Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "von6na4lM6-b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjK-ml8pM-az"
      },
      "source": [
        "# Create directory to store inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdpr1wTEM-9L"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/stance_detection_datasets/inferences', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVLgbMRsNror"
      },
      "source": [
        "# Import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VesFLoXN2_I"
      },
      "outputs": [],
      "source": [
        "constraint22_dataset_uspolitics_test = pd.read_csv('/content/drive/MyDrive/stance_detection_datasets/constraint22_dataset_uspolitics/constraint22_dataset_uspolitics_test.csv')\n",
        "constrain22_dataset_covid19_test = pd.read_csv('/content/drive/MyDrive/stance_detection_datasets/constrain22_dataset_covid19/constrain22_dataset_covid19_test.csv')\n",
        "DISARM_test_all = pd.read_csv('/content/drive/MyDrive/stance_detection_datasets/DISARM/DISARM_test_all.csv')\n",
        "total_defense_memes = pd.read_csv('/content/drive/MyDrive/stance_detection_datasets/total_defense_memes/total_defense_memes.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6icRsMhNuOL"
      },
      "source": [
        "# Balanced sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp5GP6yiNwhq"
      },
      "outputs": [],
      "source": [
        "constraint22_dataset_uspolitics_test = constraint22_dataset_uspolitics_test.groupby('role').sample(n=250, random_state=1).reset_index(drop=True)\n",
        "constrain22_dataset_covid19_test = constrain22_dataset_covid19_test.groupby('role').sample(n=190, random_state=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select unique images from **total_defense_memes**"
      ],
      "metadata": {
        "id": "359cp81txPZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_defense_memes = total_defense_memes[['image']].drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "UR-YbnK3xPrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ZqSNNeOFm1"
      },
      "source": [
        "# Set prompt and define a function to call the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHp6PaOOCfw"
      },
      "outputs": [],
      "source": [
        "# Use llava-v1.5-13b for the inference. via\n",
        "# https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py\n",
        "# https://medium.com/supportvectors/comparing-image-to-text-models-blip-blip2-and-llava-ebd66d2eb6d8\n",
        "def get_caption(image, entity):\n",
        "    image = load_image(image)\n",
        "    image_tensor = image_processor([image], return_tensors='pt')['pixel_values']\n",
        "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
        "    prompt = f'USER: <image>\\nWhat does the meme show, in particular text and entities such as {entity}? Describe in english:\\nASSISTANT:'\n",
        "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
        "    stopping_criteria = KeywordsStoppingCriteria([\"</s>\"], tokenizer, input_ids)\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=image_tensor,\n",
        "            do_sample=True,\n",
        "            temperature=0.2,\n",
        "            max_new_tokens=512,\n",
        "            use_cache=True,\n",
        "            stopping_criteria=[stopping_criteria])\n",
        "    output = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
        "    return output[:-4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use llava-v1.5-13b for the inference. via\n",
        "# https://github.com/haotian-liu/LLaVA/blob/main/llava/serve/cli.py\n",
        "# https://medium.com/supportvectors/comparing-image-to-text-models-blip-blip2-and-llava-ebd66d2eb6d8\n",
        "def get_caption_TDEF(image):\n",
        "    image = load_image(image)\n",
        "    image_tensor = image_processor([image], return_tensors='pt')['pixel_values']\n",
        "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
        "    prompt = 'USER: <image>\\nWhat does the meme show, in particular text and entities? Describe in english:\\nASSISTANT:'\n",
        "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
        "    stopping_criteria = KeywordsStoppingCriteria([\"</s>\"], tokenizer, input_ids)\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=image_tensor,\n",
        "            do_sample=True,\n",
        "            temperature=0.2,\n",
        "            max_new_tokens=512,\n",
        "            use_cache=True,\n",
        "            stopping_criteria=[stopping_criteria])\n",
        "    output = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
        "    return output[:-4]"
      ],
      "metadata": {
        "id": "sKN4VapRxRdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL5WFrudQxL1"
      },
      "source": [
        "# Call the `get_caption` and `get_caption_TDEF` functions and save inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFgw1XowtB_7"
      },
      "outputs": [],
      "source": [
        "uspolitics_test_images = constraint22_dataset_uspolitics_test['image'].values\n",
        "uspolitics_test_entities = constraint22_dataset_uspolitics_test['entity'].values\n",
        "constraint22_dataset_uspolitics_test['caption'] = [get_caption(image, entity) for image, entity in zip(uspolitics_test_images, uspolitics_test_entities)]\n",
        "constraint22_dataset_uspolitics_test.to_csv('/content/drive/MyDrive/stance_detection_datasets/inferences/constraint22_dataset_uspolitics_test_captioned_llava-v1.5-13b.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWXBdRSeMDRV"
      },
      "outputs": [],
      "source": [
        "covid19_test_images = constrain22_dataset_covid19_test['image'].values\n",
        "covid19_test_entities = constrain22_dataset_covid19_test['entity'].values\n",
        "constrain22_dataset_covid19_test['caption'] = [get_caption(image, entity) for image, entity in zip(covid19_test_images, covid19_test_entities)]\n",
        "constrain22_dataset_covid19_test.to_csv('/content/drive/MyDrive/stance_detection_datasets/inferences/constrain22_dataset_covid19_test_captioned_llava-v1.5-13b.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQZ_iY46lkNl"
      },
      "outputs": [],
      "source": [
        "DISARM_test_all_images = DISARM_test_all['image'].values\n",
        "DISARM_test_all_entities = DISARM_test_all['target'].values\n",
        "DISARM_test_all['caption'] = [get_caption(image, entity) for image, entity in zip(DISARM_test_all_images, DISARM_test_all_entities)]\n",
        "DISARM_test_all.to_csv('/content/drive/MyDrive/stance_detection_datasets/inferences/DISARM_test_all_captioned_llava-v1.5-13b.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_defense_memes_images = total_defense_memes['image'].values\n",
        "total_defense_memes['caption'] = [get_caption_TDEF(image) for image in total_defense_memes_images]\n",
        "total_defense_memes.to_csv('/content/drive/MyDrive/stance_detection_datasets/inferences/total_defense_memes_captioned_llava-v1.5-13b.csv', index=False)"
      ],
      "metadata": {
        "id": "fPSmNSsMxcg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}